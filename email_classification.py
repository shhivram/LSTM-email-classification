# -*- coding: utf-8 -*-
"""Email Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C_COBWhL3P9EHOVy85X5bd3NV23tNsfX

Primary Imports
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk # import NLTK
nltk.download('punkt') # For 
from nltk.corpus import stopwords # Get the stopwords
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer # Lemmentize
nltk.download('wordnet')
from sklearn.feature_extraction.text import TfidfVectorizer # tf-idf vectorizer

"""Data Imports"""

df = pd.read_csv('https://raw.githubusercontent.com/amankharwal/Email-spam-detection/master/emails.csv')

"""Data descriptions"""

df.head()

df.shape

df['spam'].value_counts()

"""# Preprocessing the data"""

df['text'] = [' '.join(i.split()[1:]) for i in df['text']]

df['text']

import re
corpus = []
lem = WordNetLemmatizer()
for i in df['text']:
  c = re.sub('[^A-Z,a-z]',' ',i).lower().split()
  c = [lem.lemmatize(i) for i in c if i not in set(stopwords.words('english'))]
  corpus.append(' '.join(c))

corpus[0]

df['text'][0]

"""Invoking the tf-idf vector"""

tfidf = TfidfVectorizer()

"""Creating the Bag of Words Model"""

X = tfidf.fit_transform(corpus).toarray()

y = np.array(df['spam'])

"""Train Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y)

for i in [X_train, X_test, y_train, y_test]:
  print(i.shape)

"""From empirical knowlege i'm choosing Naive Base classifier, becasue it has done a good job in text classification for me in the past"""

from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB().fit(X_train, y_train)

preds = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
confusion_matrix(y_test, preds)

accuracy_score(preds, y_test)

print(classification_report(preds, y_test))

"""The naive base model has done a fantastic job, with good accuracy, precsion and recall. 

Let me see if I can achive any better scores using an ANN model
"""

from tensorflow.keras.models import Sequential #used to contain the linear stack of layers
from tensorflow.keras.layers import Dense # Used to create the layers

ann_mdl = Sequential() 
ann_mdl.add(Dense(units = 10, input_dim = 30935, kernel_initializer = 'glorot_normal', activation= 'relu'))
ann_mdl.add(Dense(units = 8, kernel_initializer= 'glorot_normal', activation= 'relu'))
ann_mdl.add(Dense(units = 18, kernel_initializer= 'glorot_normal', activation= 'relu'))
ann_mdl.add(Dense(units = 5, kernel_initializer= 'glorot_normal', activation= 'relu'))
ann_mdl.add(Dense(units= 1, kernel_initializer='glorot_normal', activation= 'sigmoid'))
ann_mdl.compile(optimizer='Adamax', loss= 'binary_crossentropy', metrics = ['accuracy'])

ann_mdl.fit(X_train, y_train, epochs= 250, batch_size= 200, validation_split=0.3)

preds = ann_mdl.predict(X_test)

preds = (preds > 0.5)
accuracy_score(preds, y_test)

cm = (confusion_matrix(preds, y_test)*100/X_test.shape[0]).round(0)
sns.heatmap(cm, annot = True)
plt.show()

"""99% accuracy with no overfitting, That terrific.

Let me see What LSTM wired RNN has to offer on the dataset

Let me see how LSTM classifies

This preprocessing is not good enough and the model scores were also fantastic

But I have to preprocess the data in such a way that it has a sequencial fashion, Thus the word embedding



---

Pre-precessing the data for an LSTM model
"""

from tensorflow.keras.preprocessing.text import one_hot

vocab_size = 60000
oredr_rep = [one_hot(i,vocab_size) for i in corpus]

len(oredr_rep) == len(corpus)

"""Creating Word Embeddings"""

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense

x = max([len(i) for i in corpus])

padding_count = x

Word_embedding = pad_sequences(oredr_rep, padding= 'pre', maxlen= padding_count)

dims = 50
model = Sequential()
model.add(Embedding(vocab_size, input_length= padding_count , output_dim= dims))
model.add(LSTM(10))
model.add(Dense(1, activation= 'sigmoid'))
model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
print(model.summary())

import numpy as np
X = np.array(Word_embedding)
y = np.array(df['spam'])

X_train, X_test, y_train, y_test = train_test_split(X,y)

for i in [X_train, X_test, y_train, y_test]:
  print(i.shape)

model.fit(X_train, y_train, epochs = 105, batch_size = 200, validation_split = 0.25)

preds = model.predict(X_test)
preds = (preds> 0.5)

accuracy_score(preds, y_test)

cm = (confusion_matrix(preds, y_test)*100/X_test.shape[0]).round(0)
sns.heatmap(cm, annot = True)
plt.show()

print(classification_report(preds, y_test))

"""#### The LSTM model usually performs really good with sequencial data.

#### Since I'm not having enough ram i'm unable to tweak and experiment with the LSTM hyperparametrs and normal sequencial hyperparameters like num of epochs etc. 


---

---

---







# The best model is the ANN model (ann_mld) with a 99% in predicting a given email as a spam or ham.
"""

